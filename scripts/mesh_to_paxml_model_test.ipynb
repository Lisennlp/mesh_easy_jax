{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8edbd5b-fae2-4048-a6f2-a987de2cbf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 11:19:09.228836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n"
     ]
    }
   ],
   "source": [
    "from paxml import main\n",
    "import sys\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('/home/lishengping/projects/paxml/paxml')\n",
    "\n",
    "from praxis import base_hyperparams\n",
    "from praxis import base_input\n",
    "from praxis import pax_fiddle\n",
    "from praxis import py_utils\n",
    "from paxml import checkpoints  # mapped to internal\n",
    "from paxml import checkpoint_managers\n",
    "from paxml import partitioning\n",
    "from etils import epath\n",
    "from paxml.checkpoint_metadata import make_metadata\n",
    "import jax.numpy as jnp\n",
    "import copy\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict, empty_node\n",
    "from optax import MaskedNode\n",
    "import jax\n",
    "from praxis import optimizers\n",
    "from praxis import schedules\n",
    "import orbax\n",
    "from paxml import main\n",
    "from praxis import py_utils\n",
    "from paxml import train_states\n",
    "\n",
    "\n",
    "jax.distributed.initialize()\n",
    "\n",
    "TrainState = train_states.TrainState\n",
    "instantiate = base_hyperparams.instantiate\n",
    "CheckpointType = checkpoints.CheckpointType\n",
    "Checkpointer = checkpoints.Checkpointer\n",
    "PaxCheckpointHandler = checkpoints.PaxCheckpointHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8942936e-c0bd-4807-aa82-67a85b2cfefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LargeMlp`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.SmallMlp`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudTransformerAdam`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudTransformerAdamTest`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudTransformerAdamLimitSteps`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmdTest`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd2B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd2BLimitSteps`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd32B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd64B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd128B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd256B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd512B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmd1024B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmdPipeline9B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmdPipeline175B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmdMultislice2B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmdPipelineMultislice2B`\n",
      "Registered experiment `paxml.tasks.lm.params.lm_cloud.LmCloudSpmdPipelineMultislice2BCircular`\n",
      "Registered experiment `tasks.lm.params.c4.LmCloudSpmdAdam`\n",
      "Registered experiment `tasks.lm.params.c4.LmCloudSpmdAdamLimitSteps`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdAdam`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdGpt3AdamOrgHPBS1p5k1536Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdGpt3SmallRoPE`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdGpt37BRoPE`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdGpt3MediumRoPE`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdGpt3XLRoPE`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineAdam`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3AdamOrgHPBS1p5k768Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3AdamMLPerfHPBS1p5k768Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3AdamMLPerfHPBS2k512Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3AdamMLPerfHPBS3k768Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3AdamMLPerfHPBS4k1024Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3AdamMLPerfHPBS8k1024Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd1BAdam4Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd1BAdam4ReplicasLimitSteps`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd2BAdam4Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd2BAdam32Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd2BAdam32x2Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdLLaMA7BAdam32Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdLLaMA1BAdam32Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd16BAdam32Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4Spmd32BAdam64Replicas`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdGpt3L16AdamOrgHP`\n",
      "Registered experiment `tasks.lm.params.c4.C4SpmdPipelineGpt3SmallAdam8Replicas`\n"
     ]
    }
   ],
   "source": [
    "experiment_config = main.get_experiment('tasks.lm.params.c4.C4SpmdGpt37BRoPE')()\n",
    "task_p = experiment_config.task()\n",
    "jax_task = instantiate(task_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8b010e-0c36-48d9-96d9-3fa774c03e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = checkpoint_managers.CheckpointManagerOptions(\n",
    "      max_to_keep=10,\n",
    "      save_interval_steps=20,\n",
    "      cleanup_tmp_directories=True,\n",
    "  )\n",
    "\n",
    "checkpointer = Checkpointer(\n",
    "          PaxCheckpointHandler(\n",
    "              enforce_restore_shape_check=False,\n",
    "              use_ocdbt=False,\n",
    "          )\n",
    "      )\n",
    "\n",
    "job_log_dir = epath.Path('gs://llm_projects/log/lspdebug0804/checkpoints')\n",
    "checkpoint_type = CheckpointType.GDA\n",
    "\n",
    "checkpoint_manager = checkpoint_managers.OrbaxCheckpointManager(\n",
    "      job_log_dir,\n",
    "      checkpointer,\n",
    "      train_input_checkpointer=False,\n",
    "      options=options,\n",
    "      checkpoint_type=checkpoint_type,\n",
    "      tensorstore_use_ocdbt=False,\n",
    "  )\n",
    "# partitioner = partitioning.create_partitioner(\n",
    "#         jax_task,\n",
    "#         reshard_inputs=False,\n",
    "#         auto_sharding_mode=None,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e98da85-7fb4-4758-88c9-3aba594b4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 输入只是为了拿到dtype和shape的\n",
    "\n",
    "# NestedMap = py_utils.NestedMap\n",
    "\n",
    "# seed = 0\n",
    "# jax.random.PRNGKey(seed)\n",
    "\n",
    "# my_sample_input = {}\n",
    "\n",
    "# low, high = 0, 32000\n",
    "# seq_length = 10\n",
    "# # batch_size = experiment_config.PERCORE_BATCH_SIZE * 8\n",
    "# batch_size = 1\n",
    "# my_sample_input['ids'] = np.random.randint(low, high, (batch_size, seq_length)).astype(np.int32)\n",
    "# my_sample_input['labels'] = my_sample_input['ids'].astype(np.int32)\n",
    "# my_sample_input['weights'] = np.ones((batch_size, seq_length)).astype(np.float32)\n",
    "# my_sample_input['paddings'] = my_sample_input['weights']\n",
    "# my_sample_input['segment_ids'] = my_sample_input['weights'].astype(np.int32)\n",
    "# my_sample_input['segment_pos'] = np.arange(seq_length).reshape(1, -1).repeat(batch_size, axis=0).astype(np.int32)\n",
    "# my_sample_input['_seqio_provenance/shard_index'] = np.array([-1]).repeat(batch_size).astype(np.int32)\n",
    "# my_sample_input['_seqio_provenance/num_shards'] = my_sample_input['_seqio_provenance/shard_index']\n",
    "# my_sample_input['_seqio_provenance/index_within_shard'] = my_sample_input['_seqio_provenance/shard_index'].astype(np.int64)\n",
    "# my_sample_input['eval_sample_weights'] = my_sample_input['_seqio_provenance/shard_index'].astype(np.float32)\n",
    "# my_sample_input = NestedMap(my_sample_input)\n",
    "\n",
    "# inputs_shape_dtype = jax.tree_map(\n",
    "#         lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype),\n",
    "#         my_sample_input,\n",
    "#     )\n",
    "\n",
    "# partitioner.setup(\n",
    "#         jax_task,\n",
    "#         None,\n",
    "#         train_inputs_shape_dtype=inputs_shape_dtype, # None\n",
    "#         train_input_pipeline=None,\n",
    "#         job_log_dir=job_log_dir,\n",
    "#     )\n",
    "\n",
    "# train_state_metadata = partitioner.get_train_state_metadata()\n",
    "\n",
    "# items = {'state': train_state_metadata.padded_global_shapes}\n",
    "# restore_kwargs = {\n",
    "#           'version': 1.1,\n",
    "#           'specs': train_state_metadata.partition_specs, # shard\n",
    "#           'mesh': partitioner.global_mesh, # mesh\n",
    "#           'transforms': None, # None\n",
    "#       }\n",
    "# restore_kwargs = {'state': restore_kwargs}\n",
    "# restored = checkpoint_manager._manager.restore(20, items=items, restore_kwargs=restore_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "535cce1e-5389-4467-a411-293cb362a57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dda4f0-dde7-430d-9ff3-5ba5b52e6a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31db8e-2f80-4f82-b855-46767bbba2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0fd4e-6c4e-43ff-9ac3-1db3a7574aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4ccbea2-058e-4455-b1db-649c4f55063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_mngr_dir = epath.Path('gs://llm_base_models/orbax_async_test')\n",
    "gold_item = {\n",
    "                # 'opt_state': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler()),\n",
    "                'params': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler()),\n",
    "                # 'step': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.ArrayCheckpointHandler()),\n",
    "                }\n",
    "gold_mngr = orbax.checkpoint.CheckpointManager(gold_mngr_dir, gold_item)\n",
    "gold_w = gold_mngr.restore(gold_mngr.latest_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484e73f3-2721-4180-899c-ff5178b5c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "paxml_to_mesh_format = {\n",
    "        ('params', 'lm', 'embedding_lookup', 'emb_var'): 'wte',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1', 'linear', 'w'): 'w3',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1_gate', 'linear', 'w'): 'w1',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer2', 'linear', 'w'): 'w2',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'query', 'w'): 'wq',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'key', 'w'): 'wk',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'value', 'w'): 'wv',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'post', 'w'): 'wo',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'layer_norm', 'scale'): 'attention_norm',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'layer_norm', 'scale'): 'ffn_norm',\n",
    "        ('params', 'lm', 'final_ln', 'scale'): 'ln_f',\n",
    "        ('params', 'lm', 'softmax', 'logits_ffn', 'linear', 'w'): 'lm_head',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5d07316-7ab1-458d-8dc4-da5e92626c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check....\n",
      "('params', 'lm', 'embedding_lookup', 'emb_var') (32000, 4096)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1', 'linear', 'w') (2, 4096, 11008)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1_gate', 'linear', 'w') (2, 4096, 11008)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer2', 'linear', 'w') (2, 11008, 4096)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'query', 'w') (2, 4096, 32, 128)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'key', 'w') (2, 4096, 32, 128)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'value', 'w') (2, 4096, 32, 128)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'post', 'w') (2, 4096, 32, 128)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'layer_norm', 'scale') (2, 4096)\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'layer_norm', 'scale') (2, 4096)\n",
      "('params', 'lm', 'final_ln', 'scale') (4096,)\n",
      "('params', 'lm', 'softmax', 'logits_ffn', 'linear', 'w') (4096, 32000)\n"
     ]
    }
   ],
   "source": [
    "trans_result = {}\n",
    "\n",
    "num_heads = experiment_config.NUM_HEADS\n",
    "model_dims = experiment_config.MODEL_DIMS \n",
    "head_dim = model_dims // num_heads\n",
    "\n",
    "for k, v in paxml_to_mesh_format.items():\n",
    "    values = []\n",
    "    for gold_key, glod_values in flatten_dict(gold_w['params']).items():\n",
    "        if v in gold_key:\n",
    "            if v in 'wqwkwvwo':\n",
    "                glod_values = glod_values.reshape(model_dims, num_heads, head_dim)\n",
    "            values.append([gold_key, glod_values])\n",
    "    values = sorted(values, key=lambda x: x[0])\n",
    "    if len(values) > 1:\n",
    "        stack_values = np.stack(list(zip(*values))[1])\n",
    "    else:\n",
    "        stack_values = values[0][1]\n",
    "    trans_result[k] = stack_values\n",
    "opt_state_mv = jax.tree_map(lambda x: jnp.zeros_like(x), trans_result)\n",
    "\n",
    "print(f'check....')\n",
    "\n",
    "for k, v in trans_result.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b913af0a-ffc7-422b-a032-c5e0eaa38d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 200\n",
    "n_layers = 2\n",
    "\n",
    "temp_no_prefix, temp_other = {}, {}\n",
    "for key_tuple, param in opt_state_mv.items():\n",
    "    if 'repeat' in key_tuple:\n",
    "        temp_no_prefix[key_tuple] = MaskedNode()\n",
    "        temp_other[key_tuple] = param\n",
    "    else:\n",
    "        temp_no_prefix[key_tuple] = param\n",
    "        temp_other[key_tuple] = MaskedNode()\n",
    "\n",
    "temp_no_prefix = unflatten_dict(temp_no_prefix)\n",
    "temp_other = unflatten_dict(temp_other)\n",
    "    \n",
    "no_prefix = {'count': jnp.array(step), 'm': temp_no_prefix, 'v': temp_no_prefix}\n",
    "other = {'count': jnp.array([step] * n_layers), 'm': temp_other, 'v': temp_other}\n",
    "trans_opt_states = {\n",
    "                    'no_prefix': [{'count': jnp.array(step)}] * 2 + [no_prefix, {'count': jnp.array(step)}], \n",
    "                    f'p#{n_layers}#i-1': [{'count': jnp.array([step] * n_layers)}] * 2 + [other, {'count': jnp.array([step] * n_layers)}], \n",
    "}\n",
    "trans_opt_states = [trans_opt_states]\n",
    "\n",
    "new_trainstate = TrainState(step=jnp.array(step), \n",
    "                            mdl_vars=unflatten_dict(trans_result),\n",
    "                            opt_states=trans_opt_states)\n",
    "\n",
    "padded_global_shapes = jax.tree_map(lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype) \n",
    "                                    if hasattr(x, 'shape') else x , new_trainstate)\n",
    "checkpoint_manager.save(step, new_trainstate, padded_global_shapes, train_input_pipeline=None, force=False)\n",
    "print(f'Start load model to check whether saved model is True or False')\n",
    "restored_model = checkpoint_manager._manager.restore(step, items=padded_global_shapes, restore_kwargs=restore_kwargs)\n",
    "print(f'Load saved model finished. Congratulations!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a5a6f04-3763-4362-aca1-e7d204eb21a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6fc6a-f19a-4d5b-b5fd-d5acb85cb093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
