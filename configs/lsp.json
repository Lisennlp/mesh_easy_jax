{
  "save_optimizer_state": true,
  "jax_method": "flax",
  "embd_pdrop": 0.1,
  "resid_pdrop": 0.05,
  "intermediate_size": 11008,
  "transformation": "pjit", 
  "layers": 2,
  "d_model": 4096,
  "n_heads": 32,
  "n_vocab": 32000,
  "norm": "rmsnorm",
  "pe": "rotary",
  "pe_rotary_dims": 64,
  
  "seq": 2048,
  "cores_per_replica": 8,
  "per_replica_batch": 1,
  "gradient_accumulation_steps": 16,

  "warmup_steps": 3000,
  "anneal_steps": 300000,
  "lr": 1.2e-4,
  "end_lr": 1.2e-5,
  "weight_decay": 0.1,
  "total_steps": 100000,

  "tpu_size": 32,

  "bucket": "llm_base_models",
  "model_dir": "llama7b_finetune_mesh_jax_flax",

  "train_set": "gs://llm_projects/data/general_new_format_tokenized_and_concatted.tfrecords",
  "val_set": {
    "test1": "gs://llm_projects/data/general_new_format_tokenized_and_concatted.tfrecords",
    "test2": "gs://llm_projects/data/general_new_format_tokenized_and_concatted.tfrecords"
  },

  "eval_harness_tasks": [
  ],

  "val_batches": 100,
  "val_every": 50000,
  "ckpt_every": 50000,
  "keep_every": 10000,

  "name": "llama7b_finetune",
  "wandb_project": "mesh-transformer-jax",
  "comment": ""
}
