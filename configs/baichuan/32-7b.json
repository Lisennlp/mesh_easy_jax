{
    "dp": 1,
    "fsdp": 8,
    "mp": 4,
    "load_mode": "orbax",
    "save_mode": "orbax",
    "embd_pdrop": 0.1,
    "resid_pdrop": 0.05,
    "intermediate_size": 11008,
    "transformation": "pjit", 
    "num_hidden_layers":32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 64000,
    "norm": "rmsnorm",
    "rotary_from": "easylmm",
    "alibi": false,
    
    "seq": 2048,
    "cores_per_replica": 8,
    "per_replica_batch": 16,
    "gradient_accumulation_steps": 2,
    "seed": 42,
  
    "epoch_num": 3,
    "anneal_steps": 19499,
    "total_steps": 58497,
    "lr": 8e-6,
    "end_lr": 8e-7,
    "weight_decay": 0.001,
    "warmup_ratio": 0.02,
  
    "tpu_size": 32,
  
    "bucket": "llm_base_models",
    "model_dir": "baichuan-7B-easylm",
  
    "train_set": "gs://jax_llm_data/data-baichuan/dreamily_translation_general.train.tfrecords",
    "val_set": {
              "test": "gs://jax_llm_data/data-baichuan/dreamily_translation_general.test.tfrecords"
    },
  
    "val_batches": 300,
    "val_every": 1000,
    "ckpt_every": 1000,
    "keep_every": 3000,
  
    "name": "32-7b",
    "wandb_project": "baichuan-zh-llama-7b-ft"
  }